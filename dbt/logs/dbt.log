[0m16:46:26.213130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10550a3c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200339d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120033c50>]}


============================== 16:46:26.229504 | 55d4aa99-c97b-4d6f-afb8-20fb830faccd ==============================
[0m16:46:26.229504 [info ] [MainThread]: Running with dbt=1.9.1
[0m16:46:26.229731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'profiles_dir': '/Users/mvelzel/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt init doubtless', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:46:26.250632 [warn ] [MainThread]: [ConfigFolderDirectory]: Unable to parse logging event dictionary. Failed to parse dir field: expected string or bytes-like object, got 'PosixPath'.. Dictionary: {'dir': PosixPath('/Users/mvelzel/.dbt')}
[0m16:46:26.250815 [info ] [MainThread]: Creating dbt configuration folder at 
[0m16:46:26.251674 [debug] [MainThread]: Starter project path: /Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/include/starter_project
[0m16:46:26.253787 [info ] [MainThread]: 
Your new dbt project "doubtless" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m16:46:26.253905 [info ] [MainThread]: Setting up your profile.
[0m16:46:33.370816 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:46:33.371049 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:46:33.371174 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:47:23.185082 [info ] [MainThread]: Profile doubtless written to /Users/mvelzel/.dbt/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.
[0m16:47:23.189996 [debug] [MainThread]: Resource report: {"command_name": "init", "command_success": true, "command_wall_clock_time": 57.110558, "process_in_blocks": "0", "process_kernel_time": 0.218577, "process_mem_max_rss": "134905856", "process_out_blocks": "0", "process_user_time": 1.123842}
[0m16:47:23.190700 [debug] [MainThread]: Command `dbt init` succeeded at 16:47:23.190580 after 57.11 seconds
[0m16:47:23.191210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120113e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12010b650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200ff240>]}
[0m16:47:23.191747 [debug] [MainThread]: Flushing usage events
[0m16:47:24.765378 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:48:53.037880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c18ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a1bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a1bc50>]}


============================== 16:48:53.039818 | 02c57f95-391c-472c-8da1-bbbcf2a3cd04 ==============================
[0m16:48:53.039818 [info ] [MainThread]: Running with dbt=1.9.1
[0m16:48:53.040082 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:48:53.062081 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:48:53.062283 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:48:53.062388 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:48:53.071454 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "doubtless", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`
[0m16:48:53.072099 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.16468133, "process_in_blocks": "0", "process_kernel_time": 0.07549, "process_mem_max_rss": "104611840", "process_out_blocks": "0", "process_user_time": 0.440199}
[0m16:48:53.072242 [debug] [MainThread]: Command `dbt run` failed at 16:48:53.072210 after 0.16 seconds
[0m16:48:53.072363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e0e060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ee5d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112df9480>]}
[0m16:48:53.072491 [debug] [MainThread]: Flushing usage events
[0m16:48:53.524928 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:49:21.423123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1035ecad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10442bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10442bc50>]}


============================== 16:49:21.425417 | 3d53890b-c7ba-4b60-a596-f3f69a79120b ==============================
[0m16:49:21.425417 [info ] [MainThread]: Running with dbt=1.9.1
[0m16:49:21.425699 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:49:21.504925 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:49:21.505141 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:49:21.505253 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:49:21.550712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a5b490>]}
[0m16:49:21.567153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104787350>]}
[0m16:49:21.567376 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m16:49:21.599686 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m16:49:21.599995 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:49:21.600129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047ba550>]}
[0m16:49:21.992117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e3b5c0>]}
[0m16:49:22.015027 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m16:49:22.015963 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m16:49:22.031342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10430e5f0>]}
[0m16:49:22.031593 [info ] [MainThread]: Found 2 models, 4 data tests, 471 macros
[0m16:49:22.031723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d153d0>]}
[0m16:49:22.032417 [info ] [MainThread]: 
[0m16:49:22.032550 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:49:22.032653 [info ] [MainThread]: 
[0m16:49:22.032838 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m16:49:22.035091 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m16:49:22.038509 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m16:49:22.038631 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:49:22.038731 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:49:22.774565 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:49:22.774784 [debug] [ThreadPool]: SQL status: OK in 0.736 seconds
[0m16:49:22.789387 [debug] [ThreadPool]: On list_schemas: Close
[0m16:49:22.799086 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__experiments)
[0m16:49:22.799334 [debug] [ThreadPool]: Creating schema "schema: "experiments"
"
[0m16:49:22.800871 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:22.800981 [debug] [ThreadPool]: Using spark connection "create__experiments"
[0m16:49:22.801070 [debug] [ThreadPool]: On create__experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "create__experiments"} */
create schema if not exists experiments
  
[0m16:49:22.801155 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:49:23.377454 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:49:23.377697 [debug] [ThreadPool]: SQL status: OK in 0.577 seconds
[0m16:49:23.378149 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m16:49:23.378296 [debug] [ThreadPool]: On create__experiments: ROLLBACK
[0m16:49:23.378409 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:49:23.378510 [debug] [ThreadPool]: On create__experiments: Close
[0m16:49:23.382376 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__experiments, now list_None_experiments)
[0m16:49:23.384455 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:23.384599 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m16:49:23.384711 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m16:49:23.384813 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:49:23.429071 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:49:23.429276 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m16:49:23.431017 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m16:49:23.431126 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:49:23.431211 [debug] [ThreadPool]: On list_None_experiments: Close
[0m16:49:23.434213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f36390>]}
[0m16:49:23.434430 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:23.434536 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:49:23.435019 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m16:49:23.435223 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m16:49:23.436498 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m16:49:23.436627 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m16:49:23.439007 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m16:49:23.439410 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m16:49:23.445765 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m16:49:23.445886 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m16:49:23.445988 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:49:23.482184 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:49:23.482381 [debug] [Thread-1 (]: SQL status: OK in 0.036 seconds
[0m16:49:23.495372 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m16:49:23.495865 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:23.495988 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m16:49:23.496101 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:49:25.991107 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:49:25.991340 [debug] [Thread-1 (]: SQL status: OK in 2.495 seconds
[0m16:49:25.997533 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m16:49:25.997667 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:49:25.997771 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m16:49:26.002784 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104eaccb0>]}
[0m16:49:26.003073 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.57s]
[0m16:49:26.003272 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m16:49:26.003591 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m16:49:26.003818 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m16:49:26.003997 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m16:49:26.004126 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m16:49:26.005138 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m16:49:26.005397 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m16:49:26.010861 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m16:49:26.011183 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:26.011307 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m16:49:26.011424 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m16:49:26.011533 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:49:26.075919 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:49:26.076131 [debug] [Thread-1 (]: SQL status: OK in 0.065 seconds
[0m16:49:26.076942 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m16:49:26.077055 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:49:26.077152 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m16:49:26.079968 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d53890b-c7ba-4b60-a596-f3f69a79120b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050514f0>]}
[0m16:49:26.080230 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.08s]
[0m16:49:26.080420 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m16:49:26.080885 [debug] [MainThread]: On master: ROLLBACK
[0m16:49:26.081031 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:49:26.098988 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:49:26.099172 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:49:26.099267 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:49:26.099371 [debug] [MainThread]: On master: ROLLBACK
[0m16:49:26.099453 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:49:26.099536 [debug] [MainThread]: On master: Close
[0m16:49:26.102133 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:49:26.102272 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m16:49:26.102404 [info ] [MainThread]: 
[0m16:49:26.102520 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 4.07 seconds (4.07s).
[0m16:49:26.102798 [debug] [MainThread]: Command end result
[0m16:49:26.109505 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m16:49:26.110076 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m16:49:26.112012 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m16:49:26.112121 [info ] [MainThread]: 
[0m16:49:26.112255 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:49:26.112349 [info ] [MainThread]: 
[0m16:49:26.112448 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:49:26.113093 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.826657, "process_in_blocks": "0", "process_kernel_time": 0.10846, "process_mem_max_rss": "136970240", "process_out_blocks": "0", "process_user_time": 1.003966}
[0m16:49:26.113241 [debug] [MainThread]: Command `dbt run` succeeded at 16:49:26.113208 after 4.83 seconds
[0m16:49:26.113375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104354fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104abb8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104455fd0>]}
[0m16:49:26.113500 [debug] [MainThread]: Flushing usage events
[0m16:49:26.639676 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:04:21.690511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1035d8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104417b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104417c50>]}


============================== 17:04:21.692732 | d51af00c-55a0-4b44-a8df-707eb3c3f7f4 ==============================
[0m17:04:21.692732 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:04:21.693019 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:04:21.722897 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:04:21.723131 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:04:21.723253 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:04:21.767107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a3df30>]}
[0m17:04:21.784522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045f29c0>]}
[0m17:04:21.784754 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:04:21.816721 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:04:21.848779 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m17:04:21.848972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10450a450>]}
[0m17:04:22.282267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d558b0>]}
[0m17:04:22.320881 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:04:22.322526 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:04:22.330536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fef4d0>]}
[0m17:04:22.330826 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:04:22.331014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cfe820>]}
[0m17:04:22.331901 [info ] [MainThread]: 
[0m17:04:22.332132 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:04:22.332283 [info ] [MainThread]: 
[0m17:04:22.332677 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:04:22.335761 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:04:22.341126 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:04:22.341326 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:04:22.341483 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:23.432289 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:04:23.432503 [debug] [ThreadPool]: SQL status: OK in 1.091 seconds
[0m17:04:23.450386 [debug] [ThreadPool]: On list_schemas: Close
[0m17:04:23.463105 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:04:23.465164 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:04:23.465302 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:04:23.465397 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:04:23.465485 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:04:23.632317 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:04:23.632726 [debug] [ThreadPool]: SQL status: OK in 0.167 seconds
[0m17:04:23.635695 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:04:23.635883 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:04:23.635995 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:04:23.641527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d6ad50>]}
[0m17:04:23.641909 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:04:23.642042 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:04:23.644974 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m17:04:23.647221 [info ] [MainThread]: 1 of 1 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m17:04:23.647450 [debug] [MainThread]: Using spark connection "master"
[0m17:04:23.647592 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */


    

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
    create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
    create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
    create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';




[0m17:04:23.647721 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:04:23.676692 [debug] [MainThread]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'create\': extra input \'create\'.(line 7, pos 4)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */\n\n\n    \n\n    create or replace function bdd as \'com.doubtless.spark.hive.HiveBDD\';\n    create or replace function bdd_to_string as \'com.doubtless.spark.hive.HiveBDDToString\';\n----^^^\n    create or replace function bdd_and as \'com.doubtless.spark.hive.HiveBDDAnd\';\n    create or replace function bdd_or as \'com.doubtless.spark.hive.HiveBDDOr\';\n    create or replace function bdd_not as \'com.doubtless.spark.hive.HiveBDDNot\'\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'create\': extra input \'create\'.(line 7, pos 4)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */\n\n\n    \n\n    create or replace function bdd as \'com.doubtless.spark.hive.HiveBDD\';\n    create or replace function bdd_to_string as \'com.doubtless.spark.hive.HiveBDDToString\';\n----^^^\n    create or replace function bdd_and as \'com.doubtless.spark.hive.HiveBDDAnd\';\n    create or replace function bdd_or as \'com.doubtless.spark.hive.HiveBDDOr\';\n    create or replace function bdd_not as \'com.doubtless.spark.hive.HiveBDDNot\'\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:04:23.677000 [debug] [MainThread]: Spark adapter: Poll status: 5
[0m17:04:23.677156 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */


    

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
    create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
    create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
    create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';




[0m17:04:23.677370 [debug] [MainThread]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:04:23.677645 [info ] [MainThread]: 1 of 1 ERROR hook: doubtless.on-run-start.0 .................................... [[31mERROR[0m in 0.04s]
[0m17:04:23.677760 [info ] [MainThread]: 
[0m17:04:23.677876 [debug] [MainThread]: On master: ROLLBACK
[0m17:04:23.677972 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:04:23.678113 [debug] [MainThread]: On master: Close
[0m17:04:23.682999 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m17:04:23.683256 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m17:04:23.683437 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m17:04:23.683568 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m17:04:23.684875 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m17:04:23.685160 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m17:04:23.692150 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m17:04:23.692300 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m17:04:23.692414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:04:24.143027 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:04:24.143464 [debug] [Thread-1 (]: SQL status: OK in 0.451 seconds
[0m17:04:24.164901 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m17:04:24.165431 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:04:24.165638 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m17:04:24.165809 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:04:25.981075 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:04:25.981339 [debug] [Thread-1 (]: SQL status: OK in 1.815 seconds
[0m17:04:25.989234 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m17:04:25.989440 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:04:25.989567 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m17:04:25.994273 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502d020>]}
[0m17:04:25.994611 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.31s]
[0m17:04:25.994882 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m17:04:25.995191 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m17:04:25.995432 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m17:04:25.995633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m17:04:25.995757 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m17:04:25.996770 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m17:04:25.997097 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m17:04:26.002485 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m17:04:26.002852 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:04:26.002967 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m17:04:26.003083 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m17:04:26.003196 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:04:26.149966 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:04:26.150197 [debug] [Thread-1 (]: SQL status: OK in 0.147 seconds
[0m17:04:26.150822 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m17:04:26.150949 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:04:26.151051 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m17:04:26.153965 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd51af00c-55a0-4b44-a8df-707eb3c3f7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10507ca50>]}
[0m17:04:26.154262 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.16s]
[0m17:04:26.154468 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m17:04:26.154963 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:04:26.155112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:04:26.155239 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:04:26.155325 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m17:04:26.155419 [info ] [MainThread]: 
[0m17:04:26.155517 [info ] [MainThread]: Finished running 1 project hook, 1 table model, 1 view model in 0 hours 0 minutes and 3.82 seconds (3.82s).
[0m17:04:26.155877 [debug] [MainThread]: Command end result
[0m17:04:26.164465 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:04:26.165050 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:04:26.167115 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m17:04:26.167225 [info ] [MainThread]: 
[0m17:04:26.167356 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:04:26.167460 [info ] [MainThread]: 
[0m17:04:26.167654 [error] [MainThread]:   doubtless.on-run-start.0 failed, error:
 Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:04:26.168220 [info ] [MainThread]: 
[0m17:04:26.168367 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m17:04:26.169028 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 4.619165, "process_in_blocks": "0", "process_kernel_time": 0.115322, "process_mem_max_rss": "122978304", "process_out_blocks": "0", "process_user_time": 1.08047}
[0m17:04:26.169166 [debug] [MainThread]: Command `dbt run` failed at 17:04:26.169136 after 4.62 seconds
[0m17:04:26.169298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043ba950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1030aa8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104441fd0>]}
[0m17:04:26.169425 [debug] [MainThread]: Flushing usage events
[0m17:04:26.650147 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:07:41.808732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060c8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10731bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10731bc50>]}


============================== 17:07:41.811762 | 8417552a-f688-4155-9a32-3ec87112676b ==============================
[0m17:07:41.811762 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:07:41.812035 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:07:41.843311 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:07:41.843533 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:07:41.843653 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:07:41.886284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107a41f30>]}
[0m17:07:41.903660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074f69c0>]}
[0m17:07:41.903874 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:07:41.935838 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:07:41.979565 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:07:41.979723 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:07:41.996081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107de0b50>]}
[0m17:07:42.023674 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:07:42.024625 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:07:42.029533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107667e30>]}
[0m17:07:42.029703 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:07:42.029831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f0a190>]}
[0m17:07:42.030456 [info ] [MainThread]: 
[0m17:07:42.030579 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:07:42.030679 [info ] [MainThread]: 
[0m17:07:42.030857 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:07:42.032957 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:07:42.036755 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:07:42.036885 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:07:42.036980 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:42.070885 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:07:42.071111 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m17:07:42.072623 [debug] [ThreadPool]: On list_schemas: Close
[0m17:07:42.075733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:07:42.077794 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:07:42.077914 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:07:42.078004 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:07:42.078098 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:07:42.122919 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:07:42.123154 [debug] [ThreadPool]: SQL status: OK in 0.045 seconds
[0m17:07:42.124700 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:07:42.124830 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:07:42.124927 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:07:42.129816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f1bc70>]}
[0m17:07:42.130085 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:07:42.130203 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:07:42.133368 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m17:07:42.135251 [info ] [MainThread]: 1 of 1 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m17:07:42.135432 [debug] [MainThread]: Using spark connection "master"
[0m17:07:42.135557 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */


    

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
    create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
    create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
    create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';




[0m17:07:42.135674 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:07:42.167844 [debug] [MainThread]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'create\': extra input \'create\'.(line 7, pos 4)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */\n\n\n    \n\n    create or replace function bdd as \'com.doubtless.spark.hive.HiveBDD\';\n    create or replace function bdd_to_string as \'com.doubtless.spark.hive.HiveBDDToString\';\n----^^^\n    create or replace function bdd_and as \'com.doubtless.spark.hive.HiveBDDAnd\';\n    create or replace function bdd_or as \'com.doubtless.spark.hive.HiveBDDOr\';\n    create or replace function bdd_not as \'com.doubtless.spark.hive.HiveBDDNot\'\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'create\': extra input \'create\'.(line 7, pos 4)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */\n\n\n    \n\n    create or replace function bdd as \'com.doubtless.spark.hive.HiveBDD\';\n    create or replace function bdd_to_string as \'com.doubtless.spark.hive.HiveBDDToString\';\n----^^^\n    create or replace function bdd_and as \'com.doubtless.spark.hive.HiveBDDAnd\';\n    create or replace function bdd_or as \'com.doubtless.spark.hive.HiveBDDOr\';\n    create or replace function bdd_not as \'com.doubtless.spark.hive.HiveBDDNot\'\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m17:07:42.168135 [debug] [MainThread]: Spark adapter: Poll status: 5
[0m17:07:42.168275 [debug] [MainThread]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */


    

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
    create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
    create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
    create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';




[0m17:07:42.168462 [debug] [MainThread]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:07:42.168723 [info ] [MainThread]: 1 of 1 ERROR hook: doubtless.on-run-start.0 .................................... [[31mERROR[0m in 0.04s]
[0m17:07:42.168841 [info ] [MainThread]: 
[0m17:07:42.168950 [debug] [MainThread]: On master: ROLLBACK
[0m17:07:42.169032 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:07:42.169112 [debug] [MainThread]: On master: Close
[0m17:07:42.171911 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m17:07:42.172123 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m17:07:42.172277 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m17:07:42.172389 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m17:07:42.173546 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m17:07:42.173818 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m17:07:42.180578 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m17:07:42.180710 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m17:07:42.180819 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:42.230508 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:07:42.230757 [debug] [Thread-1 (]: SQL status: OK in 0.050 seconds
[0m17:07:42.245068 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m17:07:42.245522 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:07:42.245656 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m17:07:42.245781 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:07:43.415794 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:07:43.416014 [debug] [Thread-1 (]: SQL status: OK in 1.170 seconds
[0m17:07:43.423184 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m17:07:43.423326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:07:43.423437 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m17:07:43.426865 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107eed790>]}
[0m17:07:43.427111 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 1.25s]
[0m17:07:43.427291 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m17:07:43.427587 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m17:07:43.427782 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m17:07:43.427974 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m17:07:43.428101 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m17:07:43.429030 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m17:07:43.429290 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m17:07:43.434337 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m17:07:43.434594 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:07:43.434708 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m17:07:43.434824 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m17:07:43.434933 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:07:43.505303 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m17:07:43.505523 [debug] [Thread-1 (]: SQL status: OK in 0.071 seconds
[0m17:07:43.506165 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m17:07:43.506282 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m17:07:43.506385 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m17:07:43.508897 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8417552a-f688-4155-9a32-3ec87112676b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f40e10>]}
[0m17:07:43.509150 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.08s]
[0m17:07:43.509332 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m17:07:43.509792 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:07:43.509894 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:07:43.510007 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:07:43.510092 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m17:07:43.510191 [info ] [MainThread]: 
[0m17:07:43.510295 [info ] [MainThread]: Finished running 1 project hook, 1 table model, 1 view model in 0 hours 0 minutes and 1.48 seconds (1.48s).
[0m17:07:43.510645 [debug] [MainThread]: Command end result
[0m17:07:43.519068 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:07:43.519673 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:07:43.521729 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m17:07:43.521833 [info ] [MainThread]: 
[0m17:07:43.521962 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:07:43.522056 [info ] [MainThread]: 
[0m17:07:43.522248 [error] [MainThread]:   doubtless.on-run-start.0 failed, error:
 Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  	at java.base/java.lang.Thread.run(Thread.java:840)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'create': extra input 'create'.(line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
  
  
      
  
      create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
      create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
  ----^^^
      create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
      create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
      create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot'
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m17:07:43.522755 [info ] [MainThread]: 
[0m17:07:43.522860 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
[0m17:07:43.523510 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.8512286, "process_in_blocks": "0", "process_kernel_time": 0.104116, "process_mem_max_rss": "115621888", "process_out_blocks": "0", "process_user_time": 0.653776}
[0m17:07:43.523640 [debug] [MainThread]: Command `dbt run` failed at 17:07:43.523614 after 1.85 seconds
[0m17:07:43.523759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f37430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f6a180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fc5750>]}
[0m17:07:43.523870 [debug] [MainThread]: Flushing usage events
[0m17:07:44.009114 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:10:39.860213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074c8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10891bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10891bc50>]}


============================== 17:10:39.862094 | f3d98a46-45a0-486c-b270-193ea1e5db3f ==============================
[0m17:10:39.862094 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:10:39.862331 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:10:39.889999 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:10:39.890195 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:10:39.890314 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:10:39.933716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f3d98a46-45a0-486c-b270-193ea1e5db3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109041f30>]}
[0m17:10:39.951392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f3d98a46-45a0-486c-b270-193ea1e5db3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108af69c0>]}
[0m17:10:39.951617 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:10:39.983593 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:10:40.028121 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:10:40.028373 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m17:10:40.030353 [error] [MainThread]: Encountered an error:
Compilation Error
  Encountered unknown tag 'run_query'. Jinja was looking for the following tags: 'endmacro'. The innermost block that needs to be closed is 'macro'.
    line 3
      {% run_query("create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';") %}
[0m17:10:40.030974 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.25573578, "process_in_blocks": "0", "process_kernel_time": 0.117945, "process_mem_max_rss": "115752960", "process_out_blocks": "0", "process_user_time": 0.649232}
[0m17:10:40.031126 [debug] [MainThread]: Command `dbt run` failed at 17:10:40.031091 after 0.26 seconds
[0m17:10:40.031270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093e1050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093e0d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109385310>]}
[0m17:10:40.031409 [debug] [MainThread]: Flushing usage events
[0m17:10:40.539619 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:10:55.388880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103070ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103eafb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103eafc50>]}


============================== 17:10:55.391207 | f4930841-36b8-41b9-8e21-2c7bce45f867 ==============================
[0m17:10:55.391207 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:10:55.391507 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:10:55.421644 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:10:55.421877 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:10:55.421996 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:10:55.465151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044d5f30>]}
[0m17:10:55.482890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10408a9c0>]}
[0m17:10:55.483108 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:10:55.515442 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:10:55.558166 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:10:55.558424 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m17:10:55.586191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050e0b50>]}
[0m17:10:55.614046 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:10:55.615110 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:10:55.620130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040fbe30>]}
[0m17:10:55.620300 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:10:55.620411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051ea510>]}
[0m17:10:55.621016 [info ] [MainThread]: 
[0m17:10:55.621136 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:10:55.621219 [info ] [MainThread]: 
[0m17:10:55.621386 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:10:55.623441 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:10:55.627159 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:10:55.627281 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:10:55.627383 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:10:55.654544 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:10:55.654748 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m17:10:55.656137 [debug] [ThreadPool]: On list_schemas: Close
[0m17:10:55.658734 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:10:55.660771 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:10:55.660886 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:10:55.660985 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:10:55.661076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:10:55.696267 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:10:55.696498 [debug] [ThreadPool]: SQL status: OK in 0.035 seconds
[0m17:10:55.698000 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:10:55.698130 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:10:55.698228 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:10:55.700910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4930841-36b8-41b9-8e21-2c7bce45f867', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10502af70>]}
[0m17:10:55.701117 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:10:55.701231 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:10:55.705321 [debug] [MainThread]: Using spark connection "master"
[0m17:10:55.705461 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
  
[0m17:10:55.705570 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:10:55.755982 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m17:10:55.756215 [debug] [MainThread]: SQL status: OK in 0.051 seconds
[0m17:10:55.757448 [debug] [MainThread]: On master: ROLLBACK
[0m17:10:55.757557 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:10:55.757643 [debug] [MainThread]: On master: Close
[0m17:10:55.760037 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:10:55.760186 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m17:10:55.760304 [info ] [MainThread]: 
[0m17:10:55.760424 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m17:10:55.760615 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m17:10:55.761312 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5134213, "process_in_blocks": "0", "process_kernel_time": 0.099562, "process_mem_max_rss": "112230400", "process_out_blocks": "0", "process_user_time": 0.622476}
[0m17:10:55.761447 [debug] [MainThread]: Command `dbt run` failed at 17:10:55.761417 after 0.51 seconds
[0m17:10:55.761566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c8fdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105279650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105279700>]}
[0m17:10:55.761700 [debug] [MainThread]: Flushing usage events
[0m17:10:56.202496 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:12:19.179808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c34ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e1bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106e1bc50>]}


============================== 17:12:19.182056 | c87c622d-9333-4d1f-b07f-4e11e4bedb9a ==============================
[0m17:12:19.182056 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:12:19.182335 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:12:19.214163 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:12:19.214439 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:12:19.214571 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:12:19.267863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107541f30>]}
[0m17:12:19.287893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070f69c0>]}
[0m17:12:19.288268 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:12:19.323755 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:12:19.377034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:12:19.377555 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m17:12:19.415911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077dcb50>]}
[0m17:12:19.446892 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:12:19.448073 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:12:19.453865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107167e30>]}
[0m17:12:19.454095 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:12:19.454227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078f2510>]}
[0m17:12:19.454930 [info ] [MainThread]: 
[0m17:12:19.455066 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:12:19.455164 [info ] [MainThread]: 
[0m17:12:19.455357 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:12:19.457588 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:12:19.461647 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:12:19.461829 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:12:19.461949 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:12:19.493316 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:12:19.493555 [debug] [ThreadPool]: SQL status: OK in 0.032 seconds
[0m17:12:19.495102 [debug] [ThreadPool]: On list_schemas: Close
[0m17:12:19.498499 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:12:19.500755 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:12:19.500903 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:12:19.501028 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:12:19.501138 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:12:19.559361 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:12:19.559614 [debug] [ThreadPool]: SQL status: OK in 0.058 seconds
[0m17:12:19.561400 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:12:19.561544 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:12:19.561648 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:12:19.564473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c87c622d-9333-4d1f-b07f-4e11e4bedb9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10781b110>]}
[0m17:12:19.564713 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:12:19.564825 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:12:19.569452 [debug] [MainThread]: Using spark connection "master"
[0m17:12:19.569614 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    
        create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    
  
[0m17:12:19.569731 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:12:19.629672 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m17:12:19.629909 [debug] [MainThread]: SQL status: OK in 0.060 seconds
[0m17:12:19.631772 [debug] [MainThread]: On master: ROLLBACK
[0m17:12:19.631928 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:12:19.632034 [debug] [MainThread]: On master: Close
[0m17:12:19.634843 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:12:19.635011 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m17:12:19.635133 [info ] [MainThread]: 
[0m17:12:19.635259 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m17:12:19.635457 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m17:12:19.636142 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5913229, "process_in_blocks": "0", "process_kernel_time": 0.118964, "process_mem_max_rss": "116604928", "process_out_blocks": "0", "process_user_time": 0.72114}
[0m17:12:19.636301 [debug] [MainThread]: Command `dbt run` failed at 17:12:19.636265 after 0.59 seconds
[0m17:12:19.636443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106afbdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107979650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107979700>]}
[0m17:12:19.636576 [debug] [MainThread]: Flushing usage events
[0m17:12:20.143129 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:12:42.346772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109018ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab1bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab1bc50>]}


============================== 17:12:42.348746 | 631d50ad-2fd2-484b-ba99-d0bce4e7a308 ==============================
[0m17:12:42.348746 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:12:42.349028 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:12:42.377871 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:12:42.378139 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:12:42.378273 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:12:42.428863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b141f30>]}
[0m17:12:42.447809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acf69c0>]}
[0m17:12:42.448154 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:12:42.482961 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:12:42.529949 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:12:42.530262 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m17:12:42.559875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4dd350>]}
[0m17:12:42.590972 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:12:42.592539 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:12:42.598665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad6be30>]}
[0m17:12:42.598896 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:12:42.599026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5e6510>]}
[0m17:12:42.599694 [info ] [MainThread]: 
[0m17:12:42.599827 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:12:42.599927 [info ] [MainThread]: 
[0m17:12:42.600125 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:12:42.602330 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:12:42.606250 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:12:42.606402 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:12:42.606509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:12:42.640340 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:12:42.640578 [debug] [ThreadPool]: SQL status: OK in 0.034 seconds
[0m17:12:42.642422 [debug] [ThreadPool]: On list_schemas: Close
[0m17:12:42.645836 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:12:42.648250 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:12:42.648420 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:12:42.648524 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:12:42.648618 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:12:42.697713 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:12:42.698140 [debug] [ThreadPool]: SQL status: OK in 0.049 seconds
[0m17:12:42.700756 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:12:42.700950 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:12:42.701059 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:12:42.704089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '631d50ad-2fd2-484b-ba99-d0bce4e7a308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b50b2b0>]}
[0m17:12:42.704330 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:12:42.704441 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:12:42.708594 [debug] [MainThread]: Using spark connection "master"
[0m17:12:42.708767 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    
        create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    
  
[0m17:12:42.708884 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:12:42.743318 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m17:12:42.743557 [debug] [MainThread]: SQL status: OK in 0.035 seconds
[0m17:12:42.745061 [debug] [MainThread]: On master: ROLLBACK
[0m17:12:42.745231 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:12:42.745337 [debug] [MainThread]: On master: Close
[0m17:12:42.748696 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:12:42.748869 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m17:12:42.748983 [info ] [MainThread]: 
[0m17:12:42.749105 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m17:12:42.749303 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m17:12:42.750000 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.50280774, "process_in_blocks": "0", "process_kernel_time": 0.107356, "process_mem_max_rss": "114737152", "process_out_blocks": "0", "process_user_time": 0.685757}
[0m17:12:42.750157 [debug] [MainThread]: Command `dbt run` failed at 17:12:42.750121 after 0.50 seconds
[0m17:12:42.750338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7fbdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b679650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b679700>]}
[0m17:12:42.750539 [debug] [MainThread]: Flushing usage events
[0m17:12:43.212318 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:13:59.808112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107808ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108647b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108647c50>]}


============================== 17:13:59.810314 | 800a3607-bea9-4bc0-85de-6719f92e0b79 ==============================
[0m17:13:59.810314 [info ] [MainThread]: Running with dbt=1.9.1
[0m17:13:59.810579 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:13:59.842411 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:13:59.842662 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:13:59.842785 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:13:59.885854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c6df30>]}
[0m17:13:59.903647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1088229c0>]}
[0m17:13:59.903876 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m17:13:59.936139 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m17:13:59.978864 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:13:59.979117 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m17:14:00.006930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f09350>]}
[0m17:14:00.034734 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m17:14:00.035743 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m17:14:00.041076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108893e30>]}
[0m17:14:00.041248 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m17:14:00.041374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10901e510>]}
[0m17:14:00.042013 [info ] [MainThread]: 
[0m17:14:00.042144 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:14:00.042235 [info ] [MainThread]: 
[0m17:14:00.042393 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m17:14:00.044440 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m17:14:00.048034 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m17:14:00.048156 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:14:00.048249 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:14:00.072611 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:14:00.072815 [debug] [ThreadPool]: SQL status: OK in 0.025 seconds
[0m17:14:00.075133 [debug] [ThreadPool]: On list_schemas: Close
[0m17:14:00.077624 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m17:14:00.079596 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:14:00.079712 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m17:14:00.079803 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m17:14:00.079890 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:14:00.109677 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m17:14:00.109873 [debug] [ThreadPool]: SQL status: OK in 0.030 seconds
[0m17:14:00.111166 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m17:14:00.111287 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m17:14:00.111379 [debug] [ThreadPool]: On list_None_experiments: Close
[0m17:14:00.113663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800a3607-bea9-4bc0-85de-6719f92e0b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f46ea0>]}
[0m17:14:00.113836 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:14:00.113926 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:14:00.117843 [debug] [MainThread]: Using spark connection "master"
[0m17:14:00.117972 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    
        create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    
  
[0m17:14:00.118083 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:14:00.142131 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m17:14:00.142357 [debug] [MainThread]: SQL status: OK in 0.024 seconds
[0m17:14:00.143842 [debug] [MainThread]: On master: ROLLBACK
[0m17:14:00.143953 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m17:14:00.144039 [debug] [MainThread]: On master: Close
[0m17:14:00.146027 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:14:00.146162 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m17:14:00.146266 [info ] [MainThread]: 
[0m17:14:00.146385 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m17:14:00.146563 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m17:14:00.147224 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.4747716, "process_in_blocks": "0", "process_kernel_time": 0.106617, "process_mem_max_rss": "115572736", "process_out_blocks": "0", "process_user_time": 0.645721}
[0m17:14:00.147375 [debug] [MainThread]: Command `dbt run` failed at 17:14:00.147342 after 0.47 seconds
[0m17:14:00.147513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108427dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090a1650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090a1700>]}
[0m17:14:00.147643 [debug] [MainThread]: Flushing usage events
[0m17:14:00.731435 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:38:26.552878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10309cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10401bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10401bc50>]}


============================== 20:38:26.555103 | 6b6c67d7-6adf-4939-8e18-7ff916b81640 ==============================
[0m20:38:26.555103 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:38:26.555361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:38:26.592328 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:38:26.592566 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:38:26.592689 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:38:26.642148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b6c67d7-6adf-4939-8e18-7ff916b81640', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104945f30>]}
[0m20:38:26.659945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b6c67d7-6adf-4939-8e18-7ff916b81640', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042f69c0>]}
[0m20:38:26.660182 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:38:26.693015 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:38:26.737630 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:38:26.737883 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_udfs.sql
[0m20:38:26.738349 [error] [MainThread]: Encountered an error:
Compilation Error
  Encountered unknown tag 'create_bdd_udfs'. Jinja was looking for the following tags: 'endmacro'. The innermost block that needs to be closed is 'macro'.
    line 3
      {% create_bdd_udfs() %}
[0m20:38:26.738929 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.2714147, "process_in_blocks": "0", "process_kernel_time": 0.114355, "process_mem_max_rss": "113786880", "process_out_blocks": "0", "process_user_time": 0.621604}
[0m20:38:26.739075 [debug] [MainThread]: Command `dbt run` failed at 20:38:26.739042 after 0.27 seconds
[0m20:38:26.739203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bdd150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bdcd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b856d0>]}
[0m20:38:26.739324 [debug] [MainThread]: Flushing usage events
[0m20:38:27.344193 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:38:37.163563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c10ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a4fb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a4fc50>]}


============================== 20:38:37.165291 | d34e7810-0580-4cd1-8c5c-f5b5c6316f60 ==============================
[0m20:38:37.165291 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:38:37.165509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'debug': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:38:37.189456 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:38:37.189656 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:38:37.189766 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:38:37.231429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd34e7810-0580-4cd1-8c5c-f5b5c6316f60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110441f30>]}
[0m20:38:37.248733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd34e7810-0580-4cd1-8c5c-f5b5c6316f60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c2a9c0>]}
[0m20:38:37.248947 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:38:37.280354 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:38:37.321269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:38:37.321499 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_udfs.sql
[0m20:38:37.348260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd34e7810-0580-4cd1-8c5c-f5b5c6316f60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106dd450>]}
[0m20:38:37.375399 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:38:37.376878 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:38:37.382956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd34e7810-0580-4cd1-8c5c-f5b5c6316f60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106c9be30>]}
[0m20:38:37.383116 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:38:37.383226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd34e7810-0580-4cd1-8c5c-f5b5c6316f60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107e6510>]}
[0m20:38:37.383810 [info ] [MainThread]: 
[0m20:38:37.383922 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:38:37.384004 [info ] [MainThread]: 
[0m20:38:37.384151 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:38:37.386183 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:38:37.389741 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:38:37.389863 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:38:37.389959 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:38:37.391389 [error] [ThreadPool]: Could not connect to any of [('::1', 10000, 0, 0), ('127.0.0.1', 10000)]
[0m20:38:37.391550 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:38:37.391652 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m20:38:37.391760 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m20:38:37.391843 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m20:38:37.392081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:38:37.392172 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m20:38:37.392257 [info ] [MainThread]: 
[0m20:38:37.392367 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m20:38:37.392524 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m20:38:37.393240 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.36313397, "process_in_blocks": "0", "process_kernel_time": 0.08715, "process_mem_max_rss": "113557504", "process_out_blocks": "0", "process_user_time": 0.600679}
[0m20:38:37.393377 [debug] [MainThread]: Command `dbt run` failed at 20:38:37.393347 after 0.36 seconds
[0m20:38:37.393490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107cb110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10682fdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1107b9310>]}
[0m20:38:37.393599 [debug] [MainThread]: Flushing usage events
[0m20:38:37.999771 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:33.397117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070f4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f33b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f33c50>]}


============================== 20:39:33.399380 | 6650d41d-b8f9-4b8e-b260-8610c9e42b51 ==============================
[0m20:39:33.399380 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:39:33.399658 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:39:33.429887 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:39:33.430119 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:39:33.430237 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:39:33.472999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108559f30>]}
[0m20:39:33.490634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10810e9c0>]}
[0m20:39:33.490864 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:39:33.522520 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:39:33.564986 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:39:33.565234 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m20:39:33.593017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091dcb50>]}
[0m20:39:33.620574 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:39:33.621522 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:39:33.626425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108183e30>]}
[0m20:39:33.626591 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:39:33.626701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1092ea510>]}
[0m20:39:33.627332 [info ] [MainThread]: 
[0m20:39:33.627459 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:39:33.627555 [info ] [MainThread]: 
[0m20:39:33.627711 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:39:33.629836 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:39:33.633394 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:39:33.633511 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:39:33.633604 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:34.447680 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:39:34.447904 [debug] [ThreadPool]: SQL status: OK in 0.814 seconds
[0m20:39:34.466626 [debug] [ThreadPool]: On list_schemas: Close
[0m20:39:34.476975 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:39:34.479353 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:39:34.479511 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:39:34.479619 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:39:34.479715 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:39:34.748059 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:39:34.748535 [debug] [ThreadPool]: SQL status: OK in 0.269 seconds
[0m20:39:34.757283 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:39:34.757525 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:39:34.757784 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:39:34.765527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6650d41d-b8f9-4b8e-b260-8610c9e42b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10921aea0>]}
[0m20:39:34.766140 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:39:34.766462 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:39:34.776100 [debug] [MainThread]: Using spark connection "master"
[0m20:39:34.776842 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
  
[0m20:39:34.777284 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:39:34.984227 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:39:34.984864 [debug] [MainThread]: SQL status: OK in 0.207 seconds
[0m20:39:34.989275 [debug] [MainThread]: On master: ROLLBACK
[0m20:39:34.989655 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:39:34.989819 [debug] [MainThread]: On master: Close
[0m20:39:34.997910 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:39:34.998184 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m20:39:34.998339 [info ] [MainThread]: 
[0m20:39:34.998644 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 1.37 seconds (1.37s).
[0m20:39:34.999277 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m20:39:35.000708 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7411458, "process_in_blocks": "0", "process_kernel_time": 0.100852, "process_mem_max_rss": "113836032", "process_out_blocks": "0", "process_user_time": 0.625634}
[0m20:39:35.001019 [debug] [MainThread]: Command `dbt run` failed at 20:39:35.000968 after 1.74 seconds
[0m20:39:35.001201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d13dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109379700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1093797b0>]}
[0m20:39:35.001530 [debug] [MainThread]: Flushing usage events
[0m20:39:35.435254 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:45.534246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107430ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10826fb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10826fc50>]}


============================== 20:39:45.536583 | 78b3b76d-6575-47f0-b22f-89c6a7969968 ==============================
[0m20:39:45.536583 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:39:45.536957 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m20:39:45.576023 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:39:45.576370 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:39:45.576598 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:39:45.637197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a641f30>]}
[0m20:39:45.661788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10844a9c0>]}
[0m20:39:45.662209 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:39:45.721272 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:39:45.774950 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:39:45.775174 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:39:45.793925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0dcb50>]}
[0m20:39:45.824342 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:39:45.825471 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:39:45.831042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108767e30>]}
[0m20:39:45.831274 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:39:45.831401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c302190>]}
[0m20:39:45.832129 [info ] [MainThread]: 
[0m20:39:45.832275 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:39:45.832384 [info ] [MainThread]: 
[0m20:39:45.832583 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:39:45.834869 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:39:45.838893 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:39:45.839042 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:39:45.839150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:45.882280 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:39:45.882531 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m20:39:45.884169 [debug] [ThreadPool]: On list_schemas: Close
[0m20:39:45.888216 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:39:45.890505 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:39:45.890656 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:39:45.890762 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:39:45.890860 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:39:45.945112 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:39:45.945355 [debug] [ThreadPool]: SQL status: OK in 0.054 seconds
[0m20:39:45.947429 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:39:45.947603 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:39:45.947706 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:39:45.952106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '78b3b76d-6575-47f0-b22f-89c6a7969968', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c317c70>]}
[0m20:39:45.952369 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:39:45.952482 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:39:45.956934 [debug] [MainThread]: Using spark connection "master"
[0m20:39:45.957103 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
  
[0m20:39:45.957219 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:39:45.999873 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:39:46.000234 [debug] [MainThread]: SQL status: OK in 0.043 seconds
[0m20:39:46.002746 [debug] [MainThread]: On master: ROLLBACK
[0m20:39:46.002948 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:39:46.003060 [debug] [MainThread]: On master: Close
[0m20:39:46.009776 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:39:46.009991 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m20:39:46.010190 [info ] [MainThread]: 
[0m20:39:46.010507 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m20:39:46.010750 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m20:39:46.011471 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5998859, "process_in_blocks": "0", "process_kernel_time": 0.118652, "process_mem_max_rss": "115425280", "process_out_blocks": "0", "process_user_time": 0.657637}
[0m20:39:46.011645 [debug] [MainThread]: Command `dbt run` failed at 20:39:46.011609 after 0.60 seconds
[0m20:39:46.011792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10804fdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c31ce10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c31cec0>]}
[0m20:39:46.011933 [debug] [MainThread]: Flushing usage events
[0m20:39:46.513901 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:40:04.445104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103890ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046cfb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046cfc50>]}


============================== 20:40:04.447468 | e976a992-78e7-404c-99ca-a9c0a2557bfa ==============================
[0m20:40:04.447468 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:40:04.447758 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:40:04.478486 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:40:04.478714 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:40:04.478833 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:40:04.521526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cf5f30>]}
[0m20:40:04.539074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048aa9c0>]}
[0m20:40:04.539296 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:40:04.572524 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:40:04.624524 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:40:04.625106 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m20:40:04.664783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f91350>]}
[0m20:40:04.705313 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:40:04.706822 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:40:04.715843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10491be30>]}
[0m20:40:04.716088 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:40:04.716225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10509e510>]}
[0m20:40:04.717188 [info ] [MainThread]: 
[0m20:40:04.717576 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:40:04.717711 [info ] [MainThread]: 
[0m20:40:04.717990 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:40:04.721308 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:40:04.726664 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:40:04.727100 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:40:04.727578 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:04.794800 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:40:04.795112 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m20:40:04.797974 [debug] [ThreadPool]: On list_schemas: Close
[0m20:40:04.802890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:40:04.805836 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:40:04.806015 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:40:04.806125 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:40:04.806223 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:40:04.884095 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:40:04.884427 [debug] [ThreadPool]: SQL status: OK in 0.078 seconds
[0m20:40:04.887413 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:40:04.887838 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:40:04.888109 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:40:04.898012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e976a992-78e7-404c-99ca-a9c0a2557bfa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10507aea0>]}
[0m20:40:04.898544 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:40:04.898955 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:40:04.907834 [debug] [MainThread]: Using spark connection "master"
[0m20:40:04.908242 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    
        create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    
  
[0m20:40:04.908509 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:40:04.985246 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:40:04.985633 [debug] [MainThread]: SQL status: OK in 0.077 seconds
[0m20:40:04.988279 [debug] [MainThread]: On master: ROLLBACK
[0m20:40:04.988473 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:40:04.988587 [debug] [MainThread]: On master: Close
[0m20:40:04.995164 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:40:04.995776 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m20:40:04.996046 [info ] [MainThread]: 
[0m20:40:04.996362 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m20:40:04.996597 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m20:40:04.997303 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6954218, "process_in_blocks": "0", "process_kernel_time": 0.10795, "process_mem_max_rss": "115589120", "process_out_blocks": "0", "process_user_time": 0.662101}
[0m20:40:04.997558 [debug] [MainThread]: Command `dbt run` failed at 20:40:04.997498 after 0.70 seconds
[0m20:40:04.997740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044afdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105571700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055717b0>]}
[0m20:40:04.997893 [debug] [MainThread]: Flushing usage events
[0m20:40:06.892817 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:41:06.531560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105020ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e5fb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e5fc50>]}


============================== 20:41:06.535170 | 36fe2fcb-46bd-4738-b6e3-a517fe8aad40 ==============================
[0m20:41:06.535170 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:41:06.535651 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:41:06.567871 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:41:06.568147 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:41:06.568282 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:41:06.616442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106641f30>]}
[0m20:41:06.635525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10603a9c0>]}
[0m20:41:06.635854 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:41:06.670442 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:41:06.717254 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:41:06.717602 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m20:41:06.717756 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_udfs.sql
[0m20:41:06.748864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068dcb50>]}
[0m20:41:06.779242 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:41:06.780295 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:41:06.785430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060abe30>]}
[0m20:41:06.785620 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:41:06.785749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069ea510>]}
[0m20:41:06.786397 [info ] [MainThread]: 
[0m20:41:06.786529 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:41:06.786625 [info ] [MainThread]: 
[0m20:41:06.786810 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:41:06.788930 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:41:06.792601 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:41:06.792726 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:41:06.792823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:41:06.824105 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:41:06.824324 [debug] [ThreadPool]: SQL status: OK in 0.031 seconds
[0m20:41:06.825866 [debug] [ThreadPool]: On list_schemas: Close
[0m20:41:06.828805 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:41:06.830808 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:41:06.830934 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:41:06.831035 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:41:06.831127 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:41:06.874228 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:41:06.874438 [debug] [ThreadPool]: SQL status: OK in 0.043 seconds
[0m20:41:06.875861 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:41:06.876009 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:41:06.876104 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:41:06.878803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36fe2fcb-46bd-4738-b6e3-a517fe8aad40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10691b110>]}
[0m20:41:06.878976 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:41:06.879067 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:41:06.883475 [debug] [MainThread]: Using spark connection "master"
[0m20:41:06.883606 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */

    
        create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
    
  
[0m20:41:06.883713 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:41:06.912028 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:41:06.912255 [debug] [MainThread]: SQL status: OK in 0.029 seconds
[0m20:41:06.913653 [debug] [MainThread]: On master: ROLLBACK
[0m20:41:06.913773 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:41:06.913867 [debug] [MainThread]: On master: Close
[0m20:41:06.916439 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:41:06.916586 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m20:41:06.916702 [info ] [MainThread]: 
[0m20:41:06.916828 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m20:41:06.917028 [error] [MainThread]: Encountered an error:
Compilation Error in operation doubtless-on-run-start-0 (./dbt_project.yml)
  'NoneType' object is not iterable
  
  > in macro run_query (macros/etc/statement.sql)
  > called by macro create_bdd_udfs (macros/udfs/create_bdd_udfs.sql)
  > called by macro create_udfs (macros/udfs/create_udfs.sql)
  > called by operation doubtless-on-run-start-0 (./dbt_project.yml)
[0m20:41:06.917726 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.50140154, "process_in_blocks": "0", "process_kernel_time": 0.112399, "process_mem_max_rss": "116015104", "process_out_blocks": "0", "process_user_time": 0.702005}
[0m20:41:06.917866 [debug] [MainThread]: Command `dbt run` failed at 20:41:06.917833 after 0.50 seconds
[0m20:41:06.917988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c3fdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a75650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a75700>]}
[0m20:41:06.918109 [debug] [MainThread]: Flushing usage events
[0m20:41:07.395568 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:46:18.514551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039a8ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047e7b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047e7c50>]}


============================== 20:46:18.516729 | 9bfb4847-3452-4986-a3b4-72af901b9f9c ==============================
[0m20:46:18.516729 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:46:18.517006 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:46:18.546428 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:46:18.546668 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:46:18.546787 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:46:18.588934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e0df30>]}
[0m20:46:18.606197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049c29c0>]}
[0m20:46:18.606418 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:46:18.638118 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:46:18.679731 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:46:18.679987 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_udfs.sql
[0m20:46:18.680126 [debug] [MainThread]: Partial parsing: updated file: doubtless://macros/udfs/create_bdd_udfs.sql
[0m20:46:18.707326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050a9350>]}
[0m20:46:18.734490 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:46:18.735416 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:46:18.740252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a33e30>]}
[0m20:46:18.740425 [info ] [MainThread]: Found 2 models, 1 operation, 4 data tests, 473 macros
[0m20:46:18.740551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051b2510>]}
[0m20:46:18.741165 [info ] [MainThread]: 
[0m20:46:18.741293 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:46:18.741386 [info ] [MainThread]: 
[0m20:46:18.741548 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:46:18.743585 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:46:18.747193 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:46:18.747314 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:46:18.747413 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:46:18.774648 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:46:18.774872 [debug] [ThreadPool]: SQL status: OK in 0.027 seconds
[0m20:46:18.776199 [debug] [ThreadPool]: On list_schemas: Close
[0m20:46:18.778833 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:46:18.780812 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:46:18.780938 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:46:18.781032 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:46:18.781117 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:46:18.822236 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:46:18.822453 [debug] [ThreadPool]: SQL status: OK in 0.041 seconds
[0m20:46:18.824248 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:46:18.824368 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:46:18.824452 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:46:18.827150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9bfb4847-3452-4986-a3b4-72af901b9f9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050d32b0>]}
[0m20:46:18.827352 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:46:18.827450 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:46:18.830942 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m20:46:18.831359 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:46:18.831454 [debug] [MainThread]: Connection 'list_None_experiments' was properly closed.
[0m20:46:18.831559 [info ] [MainThread]: 
[0m20:46:18.831682 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.09 seconds (0.09s).
[0m20:46:18.831834 [error] [MainThread]: Encountered an error:
'list' object has no attribute 'setdefault'
[0m20:46:18.834117 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 153, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 235, in wrapper
    return func(*args, **kwargs)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 264, in wrapper
    return func(*args, **kwargs)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 328, in wrapper
    return func(*args, **kwargs)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/cli/main.py", line 578, in run
    results = task.run()
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/runnable.py", line 588, in run
    result = self.execute_with_hooks(selected_uids)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/runnable.py", line 526, in execute_with_hooks
    before_run_status = self.before_run(adapter, selected_uids)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/run.py", line 998, in before_run
    run_hooks_status = self.safe_run_hooks(adapter, RunHookType.Start, {})
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/run.py", line 900, in safe_run_hooks
    sql = self.get_hook_sql(
        adapter, hook, hook.index, num_hooks, extra_context
    )
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/run.py", line 713, in get_hook_sql
    hook_obj = get_hook(statement, index=hook_index)
  File "/Users/mvelzel/doubtless/dbt/.venv/lib/python3.13/site-packages/dbt/task/run.py", line 88, in get_hook
    hook_dict.setdefault("index", index)
    ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'setdefault'

[0m20:46:18.834994 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.45254466, "process_in_blocks": "0", "process_kernel_time": 0.096463, "process_mem_max_rss": "112984064", "process_out_blocks": "0", "process_user_time": 0.608373}
[0m20:46:18.835151 [debug] [MainThread]: Command `dbt run` failed at 20:46:18.835118 after 0.45 seconds
[0m20:46:18.835288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045c7dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105241650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105241700>]}
[0m20:46:18.835426 [debug] [MainThread]: Flushing usage events
[0m20:46:19.358403 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:47:44.286762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105370ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061afb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061afc50>]}


============================== 20:47:44.288897 | 3fa91f8c-306b-40b0-907a-e69d36c536bc ==============================
[0m20:47:44.288897 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:47:44.289156 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m20:47:44.318358 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:47:44.318585 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:47:44.318704 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:47:44.360841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067c5f30>]}
[0m20:47:44.378312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10638acf0>]}
[0m20:47:44.378529 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:47:44.409796 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:47:44.445145 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m20:47:44.445329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062a3450>]}
[0m20:47:44.987876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a97c50>]}
[0m20:47:45.031944 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:47:45.033479 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:47:45.041142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d7ea50>]}
[0m20:47:45.041407 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m20:47:45.041545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b2f790>]}
[0m20:47:45.042327 [info ] [MainThread]: 
[0m20:47:45.042480 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:47:45.042584 [info ] [MainThread]: 
[0m20:47:45.042789 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:47:45.045216 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:47:45.049150 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:47:45.049363 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:47:45.049484 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:47:45.093054 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.093400 [debug] [ThreadPool]: SQL status: OK in 0.044 seconds
[0m20:47:45.095541 [debug] [ThreadPool]: On list_schemas: Close
[0m20:47:45.100415 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:47:45.103830 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:47:45.104013 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:47:45.104124 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:47:45.104229 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:47:45.192175 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.192461 [debug] [ThreadPool]: SQL status: OK in 0.088 seconds
[0m20:47:45.195372 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:47:45.195572 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:47:45.195688 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:47:45.201186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cf1b50>]}
[0m20:47:45.201455 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:47:45.201572 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:47:45.205010 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m20:47:45.206996 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m20:47:45.207222 [debug] [MainThread]: Using spark connection "master"
[0m20:47:45.207390 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m20:47:45.207512 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:45.255949 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.256511 [debug] [MainThread]: SQL status: OK in 0.049 seconds
[0m20:47:45.257406 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.06s]
[0m20:47:45.259619 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m20:47:45.260719 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m20:47:45.261072 [debug] [MainThread]: Using spark connection "master"
[0m20:47:45.261415 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m20:47:45.350213 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.350585 [debug] [MainThread]: SQL status: OK in 0.089 seconds
[0m20:47:45.351044 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.09s]
[0m20:47:45.352197 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m20:47:45.352599 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m20:47:45.352761 [debug] [MainThread]: Using spark connection "master"
[0m20:47:45.352877 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m20:47:45.387427 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.387860 [debug] [MainThread]: SQL status: OK in 0.035 seconds
[0m20:47:45.388997 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.04s]
[0m20:47:45.390841 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m20:47:45.391404 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m20:47:45.391597 [debug] [MainThread]: Using spark connection "master"
[0m20:47:45.391724 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m20:47:45.421105 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.421358 [debug] [MainThread]: SQL status: OK in 0.029 seconds
[0m20:47:45.421820 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.03s]
[0m20:47:45.423257 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m20:47:45.423652 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m20:47:45.423813 [debug] [MainThread]: Using spark connection "master"
[0m20:47:45.423929 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m20:47:45.445445 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.445692 [debug] [MainThread]: SQL status: OK in 0.022 seconds
[0m20:47:45.446118 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m20:47:45.446254 [info ] [MainThread]: 
[0m20:47:45.446381 [debug] [MainThread]: On master: ROLLBACK
[0m20:47:45.446484 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:47:45.446575 [debug] [MainThread]: On master: Close
[0m20:47:45.450794 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m20:47:45.451246 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m20:47:45.451466 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m20:47:45.451604 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m20:47:45.452750 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m20:47:45.453112 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m20:47:45.462170 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m20:47:45.462365 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m20:47:45.462611 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:47:45.680756 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:47:45.681017 [debug] [Thread-1 (]: SQL status: OK in 0.218 seconds
[0m20:47:45.695253 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m20:47:45.695719 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:47:45.695878 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m20:47:45.696016 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:47:47.481555 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:47:47.481784 [debug] [Thread-1 (]: SQL status: OK in 1.786 seconds
[0m20:47:47.489128 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m20:47:47.489278 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:47:47.489399 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m20:47:47.492520 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dad4f0>]}
[0m20:47:47.492793 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.04s]
[0m20:47:47.492995 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m20:47:47.493249 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m20:47:47.493487 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m20:47:47.493716 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m20:47:47.493875 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m20:47:47.494764 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m20:47:47.495031 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m20:47:47.500205 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m20:47:47.500460 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:47:47.500576 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m20:47:47.500697 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m20:47:47.500796 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:47:47.591978 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:47:47.592214 [debug] [Thread-1 (]: SQL status: OK in 0.091 seconds
[0m20:47:47.592874 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m20:47:47.592992 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:47:47.593090 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m20:47:47.595684 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3fa91f8c-306b-40b0-907a-e69d36c536bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dc36b0>]}
[0m20:47:47.595953 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.10s]
[0m20:47:47.596151 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m20:47:47.596655 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:47:47.596766 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:47:47.596885 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:47:47.596974 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m20:47:47.597100 [info ] [MainThread]: 
[0m20:47:47.597207 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 2.55 seconds (2.55s).
[0m20:47:47.597643 [debug] [MainThread]: Command end result
[0m20:47:47.605442 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:47:47.606138 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:47:47.608559 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m20:47:47.608709 [info ] [MainThread]: 
[0m20:47:47.608847 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:47:47.608956 [info ] [MainThread]: 
[0m20:47:47.609069 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:47:47.609753 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.456832, "process_in_blocks": "0", "process_kernel_time": 0.11787, "process_mem_max_rss": "122437632", "process_out_blocks": "0", "process_user_time": 1.177308}
[0m20:47:47.609902 [debug] [MainThread]: Command `dbt run` succeeded at 20:47:47.609870 after 3.46 seconds
[0m20:47:47.610036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069d30d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055458d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061d9fd0>]}
[0m20:47:47.610164 [debug] [MainThread]: Flushing usage events
[0m20:47:48.100057 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:49:42.740559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103d44ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b83b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b83c50>]}


============================== 20:49:42.742847 | 3fff3c63-81c6-404a-be1f-9ea0616f330e ==============================
[0m20:49:42.742847 [info ] [MainThread]: Running with dbt=1.9.1
[0m20:49:42.743148 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m20:49:42.773498 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:49:42.773728 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:49:42.773847 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:49:42.817090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105199f30>]}
[0m20:49:42.835153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d5ecf0>]}
[0m20:49:42.835393 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m20:49:42.866661 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m20:49:42.908609 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:49:42.908782 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:49:42.925103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105450f50>]}
[0m20:49:42.952678 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:49:42.953623 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:49:42.958635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dce3f0>]}
[0m20:49:42.958809 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m20:49:42.958929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055de190>]}
[0m20:49:42.959574 [info ] [MainThread]: 
[0m20:49:42.959706 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:49:42.959799 [info ] [MainThread]: 
[0m20:49:42.959972 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m20:49:42.962052 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m20:49:42.965744 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m20:49:42.965869 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m20:49:42.966179 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:49:43.788497 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:49:43.788720 [debug] [ThreadPool]: SQL status: OK in 0.823 seconds
[0m20:49:43.807088 [debug] [ThreadPool]: On list_schemas: Close
[0m20:49:43.816949 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m20:49:43.819162 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:49:43.819309 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m20:49:43.819403 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m20:49:43.819494 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:49:43.989799 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m20:49:43.990043 [debug] [ThreadPool]: SQL status: OK in 0.171 seconds
[0m20:49:43.992379 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m20:49:43.992541 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m20:49:43.992645 [debug] [ThreadPool]: On list_None_experiments: Close
[0m20:49:43.998407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105565230>]}
[0m20:49:43.998716 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:49:43.998833 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:49:44.002526 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m20:49:44.004550 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m20:49:44.004764 [debug] [MainThread]: Using spark connection "master"
[0m20:49:44.004893 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m20:49:44.005010 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:49:44.133884 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.134151 [debug] [MainThread]: SQL status: OK in 0.129 seconds
[0m20:49:44.134590 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.14s]
[0m20:49:44.135907 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m20:49:44.136349 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m20:49:44.136507 [debug] [MainThread]: Using spark connection "master"
[0m20:49:44.136624 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m20:49:44.160636 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.160877 [debug] [MainThread]: SQL status: OK in 0.024 seconds
[0m20:49:44.161300 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.03s]
[0m20:49:44.162348 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m20:49:44.162738 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m20:49:44.162895 [debug] [MainThread]: Using spark connection "master"
[0m20:49:44.163011 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m20:49:44.187406 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.187657 [debug] [MainThread]: SQL status: OK in 0.025 seconds
[0m20:49:44.188073 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.03s]
[0m20:49:44.189111 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m20:49:44.189512 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m20:49:44.189665 [debug] [MainThread]: Using spark connection "master"
[0m20:49:44.189779 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m20:49:44.218066 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.218312 [debug] [MainThread]: SQL status: OK in 0.028 seconds
[0m20:49:44.218733 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.03s]
[0m20:49:44.220023 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m20:49:44.220422 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m20:49:44.220575 [debug] [MainThread]: Using spark connection "master"
[0m20:49:44.220690 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m20:49:44.242321 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.242545 [debug] [MainThread]: SQL status: OK in 0.022 seconds
[0m20:49:44.242941 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m20:49:44.243065 [info ] [MainThread]: 
[0m20:49:44.243180 [debug] [MainThread]: On master: ROLLBACK
[0m20:49:44.243293 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m20:49:44.243382 [debug] [MainThread]: On master: Close
[0m20:49:44.247333 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m20:49:44.247545 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m20:49:44.247713 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m20:49:44.247829 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m20:49:44.248830 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m20:49:44.249116 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m20:49:44.256127 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m20:49:44.256269 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m20:49:44.256383 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:49:44.501200 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:49:44.501447 [debug] [Thread-1 (]: SQL status: OK in 0.245 seconds
[0m20:49:44.515872 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m20:49:44.516263 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:49:44.516388 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m20:49:44.516511 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:49:46.560276 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:49:46.560561 [debug] [Thread-1 (]: SQL status: OK in 2.044 seconds
[0m20:49:46.568977 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m20:49:46.569184 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:49:46.569319 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m20:49:46.575945 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10555cf50>]}
[0m20:49:46.576328 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.33s]
[0m20:49:46.576562 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m20:49:46.577187 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m20:49:46.577465 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m20:49:46.577659 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m20:49:46.577790 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m20:49:46.578847 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m20:49:46.579172 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m20:49:46.585242 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m20:49:46.585703 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m20:49:46.585843 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m20:49:46.585969 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m20:49:46.586086 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:49:46.726917 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m20:49:46.727234 [debug] [Thread-1 (]: SQL status: OK in 0.141 seconds
[0m20:49:46.728140 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m20:49:46.728303 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m20:49:46.728424 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m20:49:46.731824 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3fff3c63-81c6-404a-be1f-9ea0616f330e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105598f70>]}
[0m20:49:46.732137 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.15s]
[0m20:49:46.732346 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m20:49:46.732805 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m20:49:46.732919 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m20:49:46.733043 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:49:46.733134 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m20:49:46.733256 [info ] [MainThread]: 
[0m20:49:46.733367 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 3.77 seconds (3.77s).
[0m20:49:46.733813 [debug] [MainThread]: Command end result
[0m20:49:46.741636 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m20:49:46.742382 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m20:49:46.744699 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m20:49:46.744830 [info ] [MainThread]: 
[0m20:49:46.744966 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:49:46.745070 [info ] [MainThread]: 
[0m20:49:46.745182 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:49:46.745879 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.1424065, "process_in_blocks": "0", "process_kernel_time": 0.103826, "process_mem_max_rss": "114393088", "process_out_blocks": "0", "process_user_time": 0.677047}
[0m20:49:46.746033 [debug] [MainThread]: Command `dbt run` succeeded at 20:49:46.746000 after 4.14 seconds
[0m20:49:46.746176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055f4af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055c2de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c32a50>]}
[0m20:49:46.746306 [debug] [MainThread]: Flushing usage events
[0m20:49:47.220478 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:41:23.556268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107318ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10871bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10871bc50>]}


============================== 21:41:23.558483 | c8ee1fce-b745-4679-aa88-53390d8173fa ==============================
[0m21:41:23.558483 [info ] [MainThread]: Running with dbt=1.9.1
[0m21:41:23.558759 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'version_check': 'True', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:41:23.588316 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:41:23.588535 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:41:23.588652 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:41:23.631872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f31f30>]}
[0m21:41:23.649743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099f6cf0>]}
[0m21:41:23.649964 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m21:41:23.681846 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m21:41:23.724320 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:41:23.724476 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:41:23.740940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1e8f50>]}
[0m21:41:23.768780 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:41:23.769764 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:41:23.775203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109a663f0>]}
[0m21:41:23.775374 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m21:41:23.775496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a376190>]}
[0m21:41:23.776142 [info ] [MainThread]: 
[0m21:41:23.776266 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:41:23.776362 [info ] [MainThread]: 
[0m21:41:23.776531 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m21:41:23.778618 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m21:41:23.782472 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m21:41:23.782605 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m21:41:23.782712 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:41:24.609638 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:41:24.609864 [debug] [ThreadPool]: SQL status: OK in 0.827 seconds
[0m21:41:24.627737 [debug] [ThreadPool]: On list_schemas: Close
[0m21:41:24.637774 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m21:41:24.639929 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:41:24.640068 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m21:41:24.640168 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m21:41:24.640260 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:41:24.818648 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:41:24.819078 [debug] [ThreadPool]: SQL status: OK in 0.179 seconds
[0m21:41:24.821889 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m21:41:24.822086 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m21:41:24.822195 [debug] [ThreadPool]: On list_None_experiments: Close
[0m21:41:24.828575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2fd230>]}
[0m21:41:24.828881 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:41:24.829019 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:41:24.832632 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m21:41:24.834596 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m21:41:24.834804 [debug] [MainThread]: Using spark connection "master"
[0m21:41:24.834929 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m21:41:24.835045 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:41:24.948029 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:41:24.948270 [debug] [MainThread]: SQL status: OK in 0.113 seconds
[0m21:41:24.948725 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.12s]
[0m21:41:24.950031 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m21:41:24.950463 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m21:41:24.950626 [debug] [MainThread]: Using spark connection "master"
[0m21:41:24.950739 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m21:41:24.975981 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:41:24.976430 [debug] [MainThread]: SQL status: OK in 0.026 seconds
[0m21:41:24.977014 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.03s]
[0m21:41:24.978073 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m21:41:24.978478 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m21:41:24.978636 [debug] [MainThread]: Using spark connection "master"
[0m21:41:24.978750 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m21:41:25.002029 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:41:25.002270 [debug] [MainThread]: SQL status: OK in 0.023 seconds
[0m21:41:25.002682 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.03s]
[0m21:41:25.003695 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m21:41:25.004093 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m21:41:25.004242 [debug] [MainThread]: Using spark connection "master"
[0m21:41:25.004355 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m21:41:25.047208 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:41:25.047436 [debug] [MainThread]: SQL status: OK in 0.043 seconds
[0m21:41:25.047838 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.04s]
[0m21:41:25.049063 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m21:41:25.049450 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m21:41:25.049604 [debug] [MainThread]: Using spark connection "master"
[0m21:41:25.049716 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m21:41:25.070146 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:41:25.070365 [debug] [MainThread]: SQL status: OK in 0.021 seconds
[0m21:41:25.070772 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m21:41:25.070904 [info ] [MainThread]: 
[0m21:41:25.071029 [debug] [MainThread]: On master: ROLLBACK
[0m21:41:25.071131 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m21:41:25.071220 [debug] [MainThread]: On master: Close
[0m21:41:25.075707 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m21:41:25.075932 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m21:41:25.076109 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m21:41:25.076231 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m21:41:25.077278 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m21:41:25.077595 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m21:41:25.084728 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:41:25.084876 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m21:41:25.084990 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:41:25.300924 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:41:25.301167 [debug] [Thread-1 (]: SQL status: OK in 0.216 seconds
[0m21:41:25.315716 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m21:41:25.316132 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:41:25.316258 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:41:25.316383 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:41:27.307568 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:41:27.308169 [debug] [Thread-1 (]: SQL status: OK in 1.992 seconds
[0m21:41:27.317790 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m21:41:27.318004 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:41:27.318129 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m21:41:27.326427 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2f4f50>]}
[0m21:41:27.326837 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.25s]
[0m21:41:27.327084 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m21:41:27.327586 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m21:41:27.327968 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m21:41:27.328428 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m21:41:27.328763 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m21:41:27.330316 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m21:41:27.330700 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m21:41:27.337864 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m21:41:27.338248 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:41:27.338371 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m21:41:27.338488 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m21:41:27.338600 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:41:27.491011 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:41:27.491369 [debug] [Thread-1 (]: SQL status: OK in 0.153 seconds
[0m21:41:27.492099 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m21:41:27.492240 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:41:27.492357 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m21:41:27.496677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8ee1fce-b745-4679-aa88-53390d8173fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a334f70>]}
[0m21:41:27.497143 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.17s]
[0m21:41:27.497367 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m21:41:27.498258 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:41:27.498562 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:41:27.498735 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:41:27.498833 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m21:41:27.498963 [info ] [MainThread]: 
[0m21:41:27.499075 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 3.72 seconds (3.72s).
[0m21:41:27.499532 [debug] [MainThread]: Command end result
[0m21:41:27.507252 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:41:27.508008 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:41:27.510364 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m21:41:27.510529 [info ] [MainThread]: 
[0m21:41:27.510673 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:41:27.510781 [info ] [MainThread]: 
[0m21:41:27.510900 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:41:27.511554 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.093019, "process_in_blocks": "0", "process_kernel_time": 0.103977, "process_mem_max_rss": "114556928", "process_out_blocks": "0", "process_user_time": 0.685236}
[0m21:41:27.511715 [debug] [MainThread]: Command `dbt run` succeeded at 21:41:27.511680 after 4.09 seconds
[0m21:41:27.511862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a38caf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a35ede0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107206a50>]}
[0m21:41:27.511995 [debug] [MainThread]: Flushing usage events
[0m21:41:28.032090 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:47:56.404609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103518ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10481bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10481bc50>]}


============================== 21:47:56.406637 | 718ad484-2a8d-4bfc-b7b2-a3f9732daf31 ==============================
[0m21:47:56.406637 [info ] [MainThread]: Running with dbt=1.9.1
[0m21:47:56.406895 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:47:56.436913 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:47:56.437177 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:47:56.437311 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:47:56.484659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f35f30>]}
[0m21:47:56.503978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049f6cf0>]}
[0m21:47:56.504345 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m21:47:56.538581 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m21:47:56.584953 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:47:56.585141 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:47:56.601783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052e8f50>]}
[0m21:47:56.629782 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:47:56.630765 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:47:56.635916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b663f0>]}
[0m21:47:56.636097 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m21:47:56.636224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105472190>]}
[0m21:47:56.636892 [info ] [MainThread]: 
[0m21:47:56.637016 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:47:56.637110 [info ] [MainThread]: 
[0m21:47:56.637284 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m21:47:56.639375 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m21:47:56.643259 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m21:47:56.643392 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m21:47:56.643501 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:47:57.092717 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.092950 [debug] [ThreadPool]: SQL status: OK in 0.449 seconds
[0m21:47:57.108793 [debug] [ThreadPool]: On list_schemas: Close
[0m21:47:57.120749 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__experiments)
[0m21:47:57.121015 [debug] [ThreadPool]: Creating schema "schema: "experiments"
"
[0m21:47:57.122944 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:57.123075 [debug] [ThreadPool]: Using spark connection "create__experiments"
[0m21:47:57.123176 [debug] [ThreadPool]: On create__experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "create__experiments"} */
create schema if not exists experiments
  
[0m21:47:57.123273 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:47:57.642368 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.642612 [debug] [ThreadPool]: SQL status: OK in 0.519 seconds
[0m21:47:57.643026 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:47:57.643146 [debug] [ThreadPool]: On create__experiments: ROLLBACK
[0m21:47:57.643250 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m21:47:57.643343 [debug] [ThreadPool]: On create__experiments: Close
[0m21:47:57.647405 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__experiments, now list_None_experiments)
[0m21:47:57.649586 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:57.649738 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m21:47:57.649853 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m21:47:57.649948 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:47:57.698549 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.698780 [debug] [ThreadPool]: SQL status: OK in 0.049 seconds
[0m21:47:57.700804 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m21:47:57.700955 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m21:47:57.701052 [debug] [ThreadPool]: On list_None_experiments: Close
[0m21:47:57.704545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053d5160>]}
[0m21:47:57.704828 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:57.704947 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:47:57.708620 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m21:47:57.710497 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m21:47:57.710675 [debug] [MainThread]: Using spark connection "master"
[0m21:47:57.710794 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m21:47:57.710917 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:47:57.769198 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.769440 [debug] [MainThread]: SQL status: OK in 0.059 seconds
[0m21:47:57.769873 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.06s]
[0m21:47:57.770961 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m21:47:57.771339 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m21:47:57.771492 [debug] [MainThread]: Using spark connection "master"
[0m21:47:57.771608 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m21:47:57.795520 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.795759 [debug] [MainThread]: SQL status: OK in 0.024 seconds
[0m21:47:57.796166 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.03s]
[0m21:47:57.797204 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m21:47:57.797744 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m21:47:57.798124 [debug] [MainThread]: Using spark connection "master"
[0m21:47:57.798368 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m21:47:57.828516 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.828764 [debug] [MainThread]: SQL status: OK in 0.030 seconds
[0m21:47:57.829190 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.03s]
[0m21:47:57.830594 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m21:47:57.830991 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m21:47:57.831149 [debug] [MainThread]: Using spark connection "master"
[0m21:47:57.831265 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m21:47:57.855922 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.856179 [debug] [MainThread]: SQL status: OK in 0.025 seconds
[0m21:47:57.856596 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.03s]
[0m21:47:57.857623 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m21:47:57.858006 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m21:47:57.858161 [debug] [MainThread]: Using spark connection "master"
[0m21:47:57.858274 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m21:47:57.880673 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.880919 [debug] [MainThread]: SQL status: OK in 0.023 seconds
[0m21:47:57.881332 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m21:47:57.881459 [info ] [MainThread]: 
[0m21:47:57.881579 [debug] [MainThread]: On master: ROLLBACK
[0m21:47:57.881679 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m21:47:57.881767 [debug] [MainThread]: On master: Close
[0m21:47:57.885679 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m21:47:57.885924 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m21:47:57.886100 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m21:47:57.886223 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m21:47:57.887431 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m21:47:57.887724 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m21:47:57.894513 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:47:57.894643 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m21:47:57.894753 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:47:57.932202 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:47:57.932447 [debug] [Thread-1 (]: SQL status: OK in 0.038 seconds
[0m21:47:57.947068 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m21:47:57.947494 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:57.947625 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:47:57.947751 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:47:59.768231 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:47:59.768493 [debug] [Thread-1 (]: SQL status: OK in 1.821 seconds
[0m21:47:59.775128 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m21:47:59.775302 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:47:59.775420 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m21:47:59.780229 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053f5850>]}
[0m21:47:59.780574 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 1.89s]
[0m21:47:59.780789 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m21:47:59.781072 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m21:47:59.781306 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m21:47:59.781552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m21:47:59.781716 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m21:47:59.783021 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m21:47:59.783367 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m21:47:59.788903 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m21:47:59.789292 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:59.789414 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m21:47:59.789534 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m21:47:59.789652 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:47:59.860921 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:47:59.861156 [debug] [Thread-1 (]: SQL status: OK in 0.071 seconds
[0m21:47:59.861791 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m21:47:59.861919 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:47:59.862026 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m21:47:59.865130 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '718ad484-2a8d-4bfc-b7b2-a3f9732daf31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054350d0>]}
[0m21:47:59.865427 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.08s]
[0m21:47:59.865624 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m21:47:59.866147 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:47:59.866287 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:47:59.866418 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:47:59.866514 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m21:47:59.866642 [info ] [MainThread]: 
[0m21:47:59.866761 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 3.23 seconds (3.23s).
[0m21:47:59.867207 [debug] [MainThread]: Command end result
[0m21:47:59.874920 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:47:59.875531 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:47:59.877693 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m21:47:59.877805 [info ] [MainThread]: 
[0m21:47:59.877926 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:47:59.878022 [info ] [MainThread]: 
[0m21:47:59.878133 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:47:59.878798 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.5660722, "process_in_blocks": "0", "process_kernel_time": 0.12977, "process_mem_max_rss": "119078912", "process_out_blocks": "0", "process_user_time": 0.794464}
[0m21:47:59.878941 [debug] [MainThread]: Command `dbt run` succeeded at 21:47:59.878911 after 3.57 seconds
[0m21:47:59.879072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054d3750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10545ee70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036fd350>]}
[0m21:47:59.879194 [debug] [MainThread]: Flushing usage events
[0m21:48:00.367621 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:58:37.776752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107918ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f1bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f1bc50>]}


============================== 21:58:37.778811 | 42dae13e-e513-499d-aefe-1d720a3d4da4 ==============================
[0m21:58:37.778811 [info ] [MainThread]: Running with dbt=1.9.1
[0m21:58:37.779066 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:58:37.811631 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:58:37.811874 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:58:37.811990 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:58:37.859653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109541f30>]}
[0m21:58:37.877378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090f6cf0>]}
[0m21:58:37.877603 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m21:58:37.909158 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m21:58:37.953480 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:58:37.953652 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:58:37.970406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1097e8f50>]}
[0m21:58:37.998353 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:58:37.999354 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:58:38.004687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091663f0>]}
[0m21:58:38.004864 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m21:58:38.004992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109972190>]}
[0m21:58:38.005654 [info ] [MainThread]: 
[0m21:58:38.005783 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:58:38.005878 [info ] [MainThread]: 
[0m21:58:38.006055 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m21:58:38.008175 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m21:58:38.011927 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m21:58:38.012042 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m21:58:38.012141 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:38.077884 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.078122 [debug] [ThreadPool]: SQL status: OK in 0.066 seconds
[0m21:58:38.079819 [debug] [ThreadPool]: On list_schemas: Close
[0m21:58:38.091859 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m21:58:38.094193 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:38.094343 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m21:58:38.094443 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m21:58:38.094536 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:58:38.270652 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.270879 [debug] [ThreadPool]: SQL status: OK in 0.176 seconds
[0m21:58:38.273376 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m21:58:38.273569 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m21:58:38.273679 [debug] [ThreadPool]: On list_None_experiments: Close
[0m21:58:38.278104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098d5230>]}
[0m21:58:38.278330 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:38.278435 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:58:38.281914 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m21:58:38.283877 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m21:58:38.284084 [debug] [MainThread]: Using spark connection "master"
[0m21:58:38.284210 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m21:58:38.284324 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:58:38.394227 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.394457 [debug] [MainThread]: SQL status: OK in 0.110 seconds
[0m21:58:38.394869 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.12s]
[0m21:58:38.396141 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m21:58:38.396556 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m21:58:38.396705 [debug] [MainThread]: Using spark connection "master"
[0m21:58:38.396815 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m21:58:38.428524 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.428927 [debug] [MainThread]: SQL status: OK in 0.032 seconds
[0m21:58:38.429536 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.03s]
[0m21:58:38.430648 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m21:58:38.431100 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m21:58:38.431261 [debug] [MainThread]: Using spark connection "master"
[0m21:58:38.431377 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m21:58:38.456331 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.456569 [debug] [MainThread]: SQL status: OK in 0.025 seconds
[0m21:58:38.456982 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.03s]
[0m21:58:38.458028 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m21:58:38.458430 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m21:58:38.458585 [debug] [MainThread]: Using spark connection "master"
[0m21:58:38.458701 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m21:58:38.486240 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.486470 [debug] [MainThread]: SQL status: OK in 0.028 seconds
[0m21:58:38.486874 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.03s]
[0m21:58:38.488136 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m21:58:38.488529 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m21:58:38.488684 [debug] [MainThread]: Using spark connection "master"
[0m21:58:38.488797 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m21:58:38.509122 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.509413 [debug] [MainThread]: SQL status: OK in 0.020 seconds
[0m21:58:38.509840 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m21:58:38.509973 [info ] [MainThread]: 
[0m21:58:38.510096 [debug] [MainThread]: On master: ROLLBACK
[0m21:58:38.510192 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m21:58:38.510281 [debug] [MainThread]: On master: Close
[0m21:58:38.514510 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m21:58:38.514771 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m21:58:38.514963 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m21:58:38.515091 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m21:58:38.516116 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m21:58:38.516422 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m21:58:38.523438 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:58:38.523571 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m21:58:38.523683 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:38.856998 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:58:38.857296 [debug] [Thread-1 (]: SQL status: OK in 0.334 seconds
[0m21:58:38.872457 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m21:58:38.872901 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:38.873041 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m21:58:38.873170 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:58:40.614260 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:58:40.614673 [debug] [Thread-1 (]: SQL status: OK in 1.741 seconds
[0m21:58:40.622840 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m21:58:40.623211 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:58:40.623880 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m21:58:40.631546 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1098f8f50>]}
[0m21:58:40.631940 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.11s]
[0m21:58:40.632173 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m21:58:40.632870 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m21:58:40.633118 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m21:58:40.633491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m21:58:40.633944 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m21:58:40.635125 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m21:58:40.635466 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m21:58:40.643237 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m21:58:40.643655 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:40.643785 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m21:58:40.643904 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m21:58:40.644017 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:40.988022 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m21:58:40.988518 [debug] [Thread-1 (]: SQL status: OK in 0.344 seconds
[0m21:58:40.989959 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m21:58:40.990367 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m21:58:40.990791 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m21:58:40.998785 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '42dae13e-e513-499d-aefe-1d720a3d4da4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109934f70>]}
[0m21:58:40.999644 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.37s]
[0m21:58:41.000217 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m21:58:41.001568 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:41.001915 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:58:41.002208 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:58:41.002717 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m21:58:41.003019 [info ] [MainThread]: 
[0m21:58:41.003562 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 3.00 seconds (3.00s).
[0m21:58:41.004737 [debug] [MainThread]: Command end result
[0m21:58:41.020455 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m21:58:41.021366 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m21:58:41.025584 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m21:58:41.025845 [info ] [MainThread]: 
[0m21:58:41.026008 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:58:41.026119 [info ] [MainThread]: 
[0m21:58:41.026237 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:58:41.027554 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.355987, "process_in_blocks": "0", "process_kernel_time": 0.122245, "process_mem_max_rss": "117964800", "process_out_blocks": "0", "process_user_time": 0.752794}
[0m21:58:41.027943 [debug] [MainThread]: Command `dbt run` succeeded at 21:58:41.027897 after 3.36 seconds
[0m21:58:41.028389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10998caf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10995ede0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107806a50>]}
[0m21:58:41.028567 [debug] [MainThread]: Flushing usage events
[0m21:58:41.498157 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:01:22.920104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103cdcad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b1bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b1bc50>]}


============================== 22:01:22.922186 | 7441fc77-245f-4d73-b309-2df82611422f ==============================
[0m22:01:22.922186 [info ] [MainThread]: Running with dbt=1.9.1
[0m22:01:22.922516 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/mvelzel/doubtless/dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/mvelzel/doubtless/dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:01:22.951374 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:01:22.951602 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:01:22.951724 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:01:22.995006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105141f30>]}
[0m22:01:23.013015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cf6cf0>]}
[0m22:01:23.013246 [info ] [MainThread]: Registered adapter: spark=1.9.0
[0m22:01:23.044963 [debug] [MainThread]: checksum: 5e8d1596cf4eae33c11286bbb248a722d21b9f00d8a7ced8137c642517055418, vars: {}, profile: , target: , version: 1.9.1
[0m22:01:23.086944 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:01:23.087103 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:01:23.103354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053e8f50>]}
[0m22:01:23.130887 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m22:01:23.131838 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m22:01:23.136873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d663f0>]}
[0m22:01:23.137048 [info ] [MainThread]: Found 2 models, 5 operations, 4 data tests, 473 macros
[0m22:01:23.137177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10556e190>]}
[0m22:01:23.137827 [info ] [MainThread]: 
[0m22:01:23.137954 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:01:23.138049 [info ] [MainThread]: 
[0m22:01:23.138221 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m22:01:23.140305 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m22:01:23.144026 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m22:01:23.144151 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m22:01:23.144258 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:01:23.637587 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m22:01:23.637836 [debug] [ThreadPool]: SQL status: OK in 0.494 seconds
[0m22:01:23.654690 [debug] [ThreadPool]: On list_schemas: Close
[0m22:01:23.665164 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_experiments)
[0m22:01:23.667395 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:01:23.667534 [debug] [ThreadPool]: Using spark connection "list_None_experiments"
[0m22:01:23.667634 [debug] [ThreadPool]: On list_None_experiments: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "list_None_experiments"} */
show table extended in experiments like '*'
  
[0m22:01:23.667724 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:01:23.831798 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m22:01:23.832202 [debug] [ThreadPool]: SQL status: OK in 0.164 seconds
[0m22:01:23.836192 [debug] [ThreadPool]: On list_None_experiments: ROLLBACK
[0m22:01:23.836410 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m22:01:23.836523 [debug] [ThreadPool]: On list_None_experiments: Close
[0m22:01:23.843983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054fd230>]}
[0m22:01:23.844326 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:01:23.844461 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:01:23.848061 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-0"
[0m22:01:23.850155 [info ] [MainThread]: 1 of 5 START hook: doubtless.on-run-start.0 .................................... [RUN]
[0m22:01:23.850378 [debug] [MainThread]: Using spark connection "master"
[0m22:01:23.850501 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd as 'com.doubtless.spark.hive.HiveBDD';
[0m22:01:23.850614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:01:23.963970 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:01:23.964203 [debug] [MainThread]: SQL status: OK in 0.114 seconds
[0m22:01:23.964625 [info ] [MainThread]: 1 of 5 OK hook: doubtless.on-run-start.0 ....................................... [[32mOK[0m in 0.12s]
[0m22:01:23.965905 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-1"
[0m22:01:23.966312 [info ] [MainThread]: 2 of 5 START hook: doubtless.on-run-start.1 .................................... [RUN]
[0m22:01:23.966473 [debug] [MainThread]: Using spark connection "master"
[0m22:01:23.966582 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_to_string as 'com.doubtless.spark.hive.HiveBDDToString';
[0m22:01:23.991623 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:01:23.991860 [debug] [MainThread]: SQL status: OK in 0.025 seconds
[0m22:01:23.992280 [info ] [MainThread]: 2 of 5 OK hook: doubtless.on-run-start.1 ....................................... [[32mOK[0m in 0.03s]
[0m22:01:23.993284 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-2"
[0m22:01:23.993674 [info ] [MainThread]: 3 of 5 START hook: doubtless.on-run-start.2 .................................... [RUN]
[0m22:01:23.993829 [debug] [MainThread]: Using spark connection "master"
[0m22:01:23.993942 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_and as 'com.doubtless.spark.hive.HiveBDDAnd';
[0m22:01:24.017364 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:01:24.017605 [debug] [MainThread]: SQL status: OK in 0.024 seconds
[0m22:01:24.018020 [info ] [MainThread]: 3 of 5 OK hook: doubtless.on-run-start.2 ....................................... [[32mOK[0m in 0.03s]
[0m22:01:24.019033 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-3"
[0m22:01:24.019413 [info ] [MainThread]: 4 of 5 START hook: doubtless.on-run-start.3 .................................... [RUN]
[0m22:01:24.019563 [debug] [MainThread]: Using spark connection "master"
[0m22:01:24.019676 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_or as 'com.doubtless.spark.hive.HiveBDDOr';
[0m22:01:24.046791 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:01:24.047018 [debug] [MainThread]: SQL status: OK in 0.027 seconds
[0m22:01:24.047422 [info ] [MainThread]: 4 of 5 OK hook: doubtless.on-run-start.3 ....................................... [[32mOK[0m in 0.03s]
[0m22:01:24.048657 [debug] [MainThread]: Writing injected SQL for node "operation.doubtless.doubtless-on-run-start-4"
[0m22:01:24.049056 [info ] [MainThread]: 5 of 5 START hook: doubtless.on-run-start.4 .................................... [RUN]
[0m22:01:24.049212 [debug] [MainThread]: Using spark connection "master"
[0m22:01:24.049323 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "connection_name": "master"} */
create or replace function bdd_not as 'com.doubtless.spark.hive.HiveBDDNot';
[0m22:01:24.070388 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m22:01:24.070618 [debug] [MainThread]: SQL status: OK in 0.021 seconds
[0m22:01:24.071027 [info ] [MainThread]: 5 of 5 OK hook: doubtless.on-run-start.4 ....................................... [[32mOK[0m in 0.02s]
[0m22:01:24.071156 [info ] [MainThread]: 
[0m22:01:24.071278 [debug] [MainThread]: On master: ROLLBACK
[0m22:01:24.071374 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m22:01:24.071463 [debug] [MainThread]: On master: Close
[0m22:01:24.075563 [debug] [Thread-1 (]: Began running node model.doubtless.my_first_dbt_model
[0m22:01:24.075782 [info ] [Thread-1 (]: 1 of 2 START sql table model experiments.my_first_dbt_model .................... [RUN]
[0m22:01:24.075939 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_experiments, now model.doubtless.my_first_dbt_model)
[0m22:01:24.076061 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_first_dbt_model
[0m22:01:24.077082 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_first_dbt_model"
[0m22:01:24.077371 [debug] [Thread-1 (]: Began executing node model.doubtless.my_first_dbt_model
[0m22:01:24.084497 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m22:01:24.084674 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */
drop table if exists experiments.my_first_dbt_model
[0m22:01:24.084802 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:01:24.318408 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m22:01:24.318652 [debug] [Thread-1 (]: SQL status: OK in 0.234 seconds
[0m22:01:24.333147 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_first_dbt_model"
[0m22:01:24.333562 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:01:24.333685 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_first_dbt_model"
[0m22:01:24.333807 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_first_dbt_model"} */

  
    
        create table experiments.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m22:01:26.501398 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m22:01:26.501675 [debug] [Thread-1 (]: SQL status: OK in 2.168 seconds
[0m22:01:26.509456 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: ROLLBACK
[0m22:01:26.509649 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m22:01:26.509785 [debug] [Thread-1 (]: On model.doubtless.my_first_dbt_model: Close
[0m22:01:26.514017 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054ecf50>]}
[0m22:01:26.514328 [info ] [Thread-1 (]: 1 of 2 OK created sql table model experiments.my_first_dbt_model ............... [[32mOK[0m in 2.44s]
[0m22:01:26.514537 [debug] [Thread-1 (]: Finished running node model.doubtless.my_first_dbt_model
[0m22:01:26.514867 [debug] [Thread-1 (]: Began running node model.doubtless.my_second_dbt_model
[0m22:01:26.515079 [info ] [Thread-1 (]: 2 of 2 START sql view model experiments.my_second_dbt_model .................... [RUN]
[0m22:01:26.515313 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.doubtless.my_first_dbt_model, now model.doubtless.my_second_dbt_model)
[0m22:01:26.515465 [debug] [Thread-1 (]: Began compiling node model.doubtless.my_second_dbt_model
[0m22:01:26.516552 [debug] [Thread-1 (]: Writing injected SQL for node "model.doubtless.my_second_dbt_model"
[0m22:01:26.516838 [debug] [Thread-1 (]: Began executing node model.doubtless.my_second_dbt_model
[0m22:01:26.522477 [debug] [Thread-1 (]: Writing runtime sql for node "model.doubtless.my_second_dbt_model"
[0m22:01:26.522792 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:01:26.522910 [debug] [Thread-1 (]: Using spark connection "model.doubtless.my_second_dbt_model"
[0m22:01:26.523026 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.1", "profile_name": "doubtless", "target_name": "dev", "node_id": "model.doubtless.my_second_dbt_model"} */
create or replace view experiments.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from experiments.my_first_dbt_model
where id = 1

[0m22:01:26.523140 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:01:26.648897 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m22:01:26.649377 [debug] [Thread-1 (]: SQL status: OK in 0.126 seconds
[0m22:01:26.650160 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: ROLLBACK
[0m22:01:26.650303 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m22:01:26.650419 [debug] [Thread-1 (]: On model.doubtless.my_second_dbt_model: Close
[0m22:01:26.653967 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7441fc77-245f-4d73-b309-2df82611422f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105534f70>]}
[0m22:01:26.654333 [info ] [Thread-1 (]: 2 of 2 OK created sql view model experiments.my_second_dbt_model ............... [[32mOK[0m in 0.14s]
[0m22:01:26.654544 [debug] [Thread-1 (]: Finished running node model.doubtless.my_second_dbt_model
[0m22:01:26.655071 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:01:26.655197 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:01:26.655327 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:01:26.655422 [debug] [MainThread]: Connection 'model.doubtless.my_second_dbt_model' was properly closed.
[0m22:01:26.655559 [info ] [MainThread]: 
[0m22:01:26.655675 [info ] [MainThread]: Finished running 5 project hooks, 1 table model, 1 view model in 0 hours 0 minutes and 3.52 seconds (3.52s).
[0m22:01:26.656136 [debug] [MainThread]: Command end result
[0m22:01:26.664136 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/mvelzel/doubtless/dbt/target/manifest.json
[0m22:01:26.664852 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/mvelzel/doubtless/dbt/target/semantic_manifest.json
[0m22:01:26.667159 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/mvelzel/doubtless/dbt/target/run_results.json
[0m22:01:26.667282 [info ] [MainThread]: 
[0m22:01:26.667415 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:01:26.667518 [info ] [MainThread]: 
[0m22:01:26.667627 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m22:01:26.668275 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.8695927, "process_in_blocks": "0", "process_kernel_time": 0.114441, "process_mem_max_rss": "116473856", "process_out_blocks": "0", "process_user_time": 0.728183}
[0m22:01:26.668432 [debug] [MainThread]: Command `dbt run` succeeded at 22:01:26.668397 after 3.87 seconds
[0m22:01:26.668576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10558caf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10555ede0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103bc6a50>]}
[0m22:01:26.668709 [debug] [MainThread]: Flushing usage events
[0m22:01:27.364527 [debug] [MainThread]: An error was encountered while trying to flush usage events
